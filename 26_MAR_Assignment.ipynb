{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Simple Linear Regression:\n",
    "Simple linear regression involves only one independent variable to predict the dependent variable. It assumes a linear relationship between the independent and dependent variables, which means the relationship can be represented by a straight line. The equation for simple linear regression can be written as:\n",
    "\n",
    "y = b0 + b1x\n",
    "\n",
    "Here, y is the dependent variable, x is the independent variable, b0 is the intercept, and b1 is the slope coefficient that represents the change in the dependent variable for a unit change in the independent variable.\n",
    "\n",
    "Example of Simple Linear Regression:\n",
    "Let's say we want to study the relationship between the number of hours studied (x) and the exam score (y). We collect data from a group of students and perform simple linear regression. The equation might look like this:\n",
    "\n",
    "y = 50 + 10x\n",
    "\n",
    "This equation suggests that for every additional hour of study (x), the expected increase in the exam score (y) is 10 units. The intercept value of 50 indicates that even with zero hours of study, the expected exam score is 50.\n",
    "\n",
    "* Multiple Linear Regression:\n",
    "Multiple linear regression extends the concept of simple linear regression by considering two or more independent variables to predict the dependent variable. It assumes a linear relationship between the dependent variable and each independent variable, while also accounting for the interaction between multiple independent variables. The equation for multiple linear regression can be written as:\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + ... + bnxn\n",
    "\n",
    "Here, y is the dependent variable, x1, x2, ..., xn are the independent variables, b0 is the intercept, and b1, b2, ..., bn are the respective slope coefficients.\n",
    "\n",
    "Example of Multiple Linear Regression:\n",
    "Let's consider an example where we want to predict the price of a house (y) based on its size (x1), number of bedrooms (x2), and distance to the city center (x3). We collect data on various houses and perform multiple linear regression. The equation might look like this:\n",
    "\n",
    "y = 50,000 + 100x1 + 10,000x2 - 500x3\n",
    "\n",
    "This equation suggests that the price of the house is influenced by all three independent variables. For example, for every unit increase in the house size (x1), we expect the price (y) to increase by 100 units. Similarly, an additional bedroom (x2) adds 10,000 units to the price, while moving one unit away from the city center (x3) reduces the price by 500 units. The intercept value of 50,000 represents the expected price when all independent variables are zero.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Linearity:\n",
    "\n",
    " The relationship between the independent and dependent variables is assumed to be linear. This means that the change in the dependent variable is proportional to the change in the independent variable(s).\n",
    "\n",
    "2. Independence: \n",
    "\n",
    "The observations in the dataset are assumed to be independent of each other. There should be no correlation or dependency among the residuals. This assumption can be evaluated by examining the dataset's collection process and ensuring that there are no inherent patterns or dependencies in the data.\n",
    "\n",
    "3. Homoscedasticity:\n",
    "\n",
    " Homoscedasticity assumes that the residuals have a constant variance across all levels of the independent variables. In other words, the spread of the residuals should be consistent across the range of predicted values.\n",
    " \n",
    "4. Normality: \n",
    "\n",
    "The residuals should follow a normal distribution. Normality is important for hypothesis testing, confidence intervals, and obtaining reliable p-values. You can check this assumption by examining the histogram, Q-Q plot, or performing a normality test (e.g., Shapiro-Wilk test, Kolmogorov-Smirnov test) on the residuals.\n",
    "\n",
    "5. No multicollinearity: \n",
    "\n",
    "In multiple linear regression, the independent variables should not be highly correlated with each other. Multicollinearity can lead to inflated standard errors and instability in the coefficient estimates. To assess multicollinearity, you can calculate the correlation matrix between the independent variables and look for strong correlations. \n",
    "\n",
    "* To check whether these assumptions hold in a given dataset, various diagnostic techniques and statistical tests can be employed. Visual inspection of plots, such as scatter plots, residual plots, and Q-Q plots, can provide insights into the linearity, homoscedasticity, and normality assumptions. Additionally, hypothesis tests, correlation analysis, and variance inflation factor calculations can help assess independence and multicollinearity assumptions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Intercept (b0):\n",
    "The intercept represents the predicted value of the dependent variable when all independent variables are set to zero. It is the value on the y-axis where the regression line crosses. The intercept captures the baseline or starting point of the relationship between the variables.\n",
    "\n",
    "Example interpretation:\n",
    "\n",
    "Let's consider a real-world scenario of predicting monthly electricity bills based on the number of appliances used. If the intercept is 50, it means that when the number of appliances is zero, the predicted monthly electricity bill is $50. This could represent a fixed cost or a minimum charge that remains irrespective of appliance usage.\n",
    "\n",
    "2. Slope (b1):\n",
    "The slope represents the change in the dependent variable associated with a one-unit change in the independent variable. It reflects the rate of change or the impact of the independent variable on the dependent variable.\n",
    "\n",
    "Example interpretation:\n",
    "\n",
    "Continuing with the electricity bill example, if the slope is 10, it means that for every additional appliance used, the predicted monthly electricity bill increases by $10. This indicates that each appliance, on average, contributes to a $10 increase in the bill.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* The main idea behind gradient descent is to iteratively update the parameters of a model by moving in the direction of steepest descent (i.e., the negative gradient) of the cost function. The goal is to find the optimal set of parameter values that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "\n",
    "* The steps involved in gradient descent are as follows:\n",
    "\n",
    "1. Initialization: Initialize the model's parameters with some initial values.\n",
    "\n",
    "2. Forward Propagation: Calculate the predicted values of the model using the current parameter values.\n",
    "\n",
    "3. Calculate the Loss: Compute the loss or cost function, which quantifies the difference between the predicted values and the actual values.\n",
    "\n",
    "4. Backward Propagation: Calculate the gradients of the parameters with respect to the loss function. This step involves calculating the partial derivatives of the cost function with respect to each parameter.\n",
    "\n",
    "5. Parameter Update: Update the parameter values by subtracting a fraction of the gradients from the current parameter values. The fraction is determined by the learning rate, which controls the step size taken in each iteration.\n",
    "\n",
    "6. Repeat: Repeat steps 2-5 until convergence or a maximum number of iterations is reached.\n",
    "\n",
    "\n",
    "\n",
    "* Gradient descent is a fundamental optimization algorithm in machine learning because it allows models to learn and adapt from data by iteratively updating their parameters. It is an essential component of training complex models with a large number of parameters, such as deep neural networks, where finding the optimal parameter values analytically is often infeasible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multiple linear regression is an extension of simple linear regression that allows for the analysis of the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the dependent variable and each independent variable, while also accounting for the interaction and combined effect of multiple independent variables on the dependent variable.\n",
    "\n",
    "* The multiple linear regression model can be represented by the following equation:\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + ... + bnxn + ε\n",
    "\n",
    "* Here, y represents the dependent variable, x1, x2, ..., xn are the independent variables, b0 is the intercept, b1, b2, ..., bn are the respective slope coefficients, and ε represents the error term. The error term captures the unexplained variation in the dependent variable that is not accounted for by the independent variables.\n",
    "\n",
    "* The key difference between multiple linear regression and simple linear regression is the number of independent variables considered. In simple linear regression, only one independent variable is used to predict the dependent variable. However, in multiple linear regression, two or more independent variables are incorporated to predict the dependent variable.\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multicollinearity refers to a high degree of correlation or linear relationship between two or more independent variables in a multiple linear regression model. It occurs when the independent variables are not independent of each other, which can lead to issues in the model estimation and interpretation. Multicollinearity can make it challenging to determine the individual effects of the correlated variables on the dependent variable and can result in unstable and unreliable coefficient estimates.\n",
    "\n",
    "* Detecting Multicollinearity:\n",
    "\n",
    "1. Correlation Matrix:.\n",
    "\n",
    " Calculate the correlation coefficients between all pairs of independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "2. Variance Inflation Factor (VIF):\n",
    "\n",
    " Compute the VIF for each independent variable. VIF measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity. Generally, VIF values greater than 5 or 10 indicate high multicollinearity.\n",
    "\n",
    "3. Tolerance: \n",
    "\n",
    "The tolerance value is the reciprocal of the VIF. Low tolerance values (close to zero) indicate high multicollinearity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Addressing Multicollinearity:\n",
    "\n",
    "\n",
    "1. Feature Selection:\n",
    "\n",
    " Remove one or more independent variables that are highly correlated with each other. Choose the most relevant variables based on their significance, relevance to the research question, or domain knowledge.\n",
    "\n",
    "2. Data Collection:\n",
    "\n",
    " Collect additional data to reduce multicollinearity. By increasing the sample size, the chances of encountering multicollinearity decrease.\n",
    "\n",
    "3. Data Transformation:\n",
    "\n",
    " Apply transformations to the variables to reduce the correlation. This could involve standardization, normalization, or using mathematical functions such as logarithmic or power transformations.\n",
    "\n",
    "4. Ridge Regression:\n",
    "\n",
    " Ridge regression is a regularization technique that can mitigate the effects of multicollinearity. It adds a penalty term to the loss function, which reduces the coefficients and accounts for multicollinearity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Polynomial regression is an extension of linear regression that allows for modeling nonlinear relationships between the independent and dependent variables. While linear regression assumes a linear relationship between the variables, polynomial regression captures more complex relationships by introducing polynomial terms (i.e., powers and combinations) of the independent variables.\n",
    "\n",
    "In polynomial regression, the model can take the following form:\n",
    "\n",
    "y = b0 + b1*x + b2*x^2 + ... + bn*x^n + ε\n",
    "\n",
    "Here, y represents the dependent variable, x is the independent variable, b0, b1, ..., bn are the coefficients, ε is the error term, and n is the degree of the polynomial.\n",
    "\n",
    "* The key difference between polynomial regression and linear regression lies in the relationship between the variables. In linear regression, the relationship is assumed to be a straight line, whereas in polynomial regression, the relationship can take the form of curves or more intricate patterns. By including polynomial terms of higher degrees (e.g., quadratic, cubic), polynomial regression can capture these nonlinear relationships.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1. Flexibility: Polynomial regression can capture nonlinear relationships between variables, allowing for more flexible modeling. It can fit curved patterns, bends, and fluctuations in the data, which linear regression may not be able to capture.\n",
    "\n",
    "2. Improved Fit: When the relationship between the variables is curvilinear or nonlinear, polynomial regression can provide a better fit to the data compared to linear regression. It can account for more complex patterns and capture the intricacies of the relationship.\n",
    "\n",
    "* Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1. Overfitting: Polynomial regression can be prone to overfitting, especially when higher-degree polynomial terms are included. Overfitting occurs when the model fits the training data too closely, leading to poor generalization to new, unseen data. Regularization techniques like ridge regression or cross-validation can help mitigate overfitting.\n",
    "\n",
    "2. Increased Complexity: As the degree of the polynomial increases, the model becomes more complex and harder to interpret. The inclusion of higher-degree terms can lead to multicollinearity and make it challenging to identify the precise impact of each variable on the dependent variable.\n",
    "\n",
    "* Situations where Polynomial Regression is preferred:\n",
    "\n",
    "1. Nonlinear Relationships: When the relationship between the variables is expected to be nonlinear or curvilinear, polynomial regression is a suitable choice. It can capture the curvature and provide a better fit to the data compared to linear regression.\n",
    "\n",
    "2. Feature Engineering: Polynomial regression can be used as a feature engineering technique to transform variables. By adding polynomial terms to the model, interactions and complex relationships can be explored, enhancing the model's ability to capture underlying patterns in the data.\n",
    "\n",
    "3. Limited Data Points: Polynomial regression may be preferred when the dataset has a limited number of data points. In such cases, polynomial regression can help create a flexible model that captures the available data points more accurately, even if a linear relationship is not apparent.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
