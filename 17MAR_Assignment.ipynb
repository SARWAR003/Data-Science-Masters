{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of a particular value for one or more variables in a given observation or record. These missing values can occur due to various reasons, such as human errors during data collection, equipment malfunction, or intentional non-responses in surveys."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values is crucial for several reasons:\n",
    "\n",
    "1. Biased or incomplete analysis: If missing values are not appropriately handled, they can lead to biased or incomplete analysis. Ignoring missing values can introduce errors in statistical analysis, data modeling, and machine learning algorithms, leading to inaccurate results.\n",
    "\n",
    "2. Reduced sample size: Missing values reduce the effective sample size, which can impact the reliability and validity of statistical inferences. The loss of data can hinder the ability to draw meaningful conclusions and make accurate predictions.\n",
    "\n",
    "3. Distorted relationships: Missing values can distort the relationships between variables. Correlations, patterns, and trends may be misleading if they are based on incomplete data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some algorithms that are not affected by missing values include:\n",
    "\n",
    "1. Decision trees: Decision tree-based algorithms, such as Random Forest and Gradient Boosting, can handle missing values naturally. They can create splits and make predictions without explicitly imputing missing values.\n",
    "\n",
    "2. Naive Bayes: Naive Bayes algorithms assume that features are conditionally independent given the class label. As a result, missing values do not affect their calculations, and they can still make predictions.\n",
    "\n",
    "3. Association rule mining: Algorithms like Apriori for association rule mining do not consider missing values since they primarily focus on identifying frequent itemsets and generating association rules.\n",
    "\n",
    "4. Support Vector Machines (SVM): SVM algorithms can handle missing values by ignoring the missing instances during training. However, they require proper imputation if the missing values are in the feature values themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removal of Missing Values (Deletion):\n",
    "\n",
    "This approach involves removing observations or variables with missing values from the dataset. There are two common strategies:\n",
    "\n",
    "* Listwise Deletion: Delete entire rows with missing values.\n",
    "* Pairwise Deletion: Delete only the specific missing values when performing calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [6, None, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Listwise deletion\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Mean/Mode/Median Imputation:\n",
    "\n",
    "In this approach, missing values are replaced with the mean, mode, or median of the respective variable. It assumes that the missing values are similar to the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B  A_imputed\n",
      "0  1.0   6.0        1.0\n",
      "1  2.0   NaN        2.0\n",
      "2  NaN   8.0        3.0\n",
      "3  4.0   NaN        4.0\n",
      "4  5.0  10.0        5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [6, None, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation for column A\n",
    "mean_A = df['A'].mean()\n",
    "df['A_imputed'] = df['A'].fillna(mean_A)\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Forward/Backward Fill (Carry Forward):\n",
    "\n",
    "This method involves filling missing values with the previous or next observed value in the dataset. It assumes a temporal or sequential relationship between the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B  A_ffill\n",
      "0  1.0   6.0      1.0\n",
      "1  NaN   NaN      1.0\n",
      "2  3.0   NaN      3.0\n",
      "3  NaN   9.0      3.0\n",
      "4  5.0  10.0      5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample DataFrame with missing values\n",
    "data = {'A': [1, None, 3, None, 5],\n",
    "        'B': [6, None, None, 9, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Forward fill for column A\n",
    "df['A_ffill'] = df['A'].fillna(method='ffill')\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regression Imputation: \n",
    "\n",
    "\n",
    "This technique utilizes regression models to predict missing values based on other variables. A regression model is trained using observations without missing values, and then the model is used to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [6, None, 8, None, 10]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in column B using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['B_imputed'] = imputer.fit_transform(df[['B']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the observations without missing values for training the regression model\n",
    "X_train = df.dropna()[['A']]\n",
    "y_train = df.dropna()['B_imputed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a LinearRegression model using the available data\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B  B_imputed  B_regression\n",
      "0  1.0   6.0        6.0           NaN\n",
      "1  2.0   NaN        8.0           7.0\n",
      "2  NaN   8.0        8.0           NaN\n",
      "3  4.0   NaN        8.0           9.0\n",
      "4  5.0  10.0       10.0           NaN\n"
     ]
    }
   ],
   "source": [
    "# Predict the missing values in column B using the trained model and fill them in the DataFrame\n",
    "X_pred = df[df['B'].isnull()][['A']]  \n",
    "df.loc[df['B'].isnull(), 'B_regression'] = reg.predict(X_pred)  # Fill in the missing values\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Multiple Imputation:\n",
    "\n",
    "Multiple imputation creates multiple plausible imputations by estimating missing values based on observed data and their uncertainty. It captures the variability and uncertainty associated with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "2  3.0   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Creating a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [6, None, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Multiple imputation using IterativeImputer\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "Imbalanced data refers to a situation where the distribution of classes or categories in a dataset is not equal or roughly equal. It means that one class or category has significantly more instances than the others. For example, in a binary classification problem, if 90% of the samples belong to Class A and only 10% belong to Class B, the data is imbalanced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If imbalanced data is not handled appropriately, it can lead to several issues:\n",
    "\n",
    "1. Biased Model Performance: When a machine learning model is trained on imbalanced data, it tends to favor the majority class. The model's performance metrics, such as accuracy, may appear high because it correctly predicts the majority class most of the time. However, the model's ability to correctly classify the minority class is usually poor.\n",
    "\n",
    "2. Misleading Evaluation: Traditional evaluation metrics, such as accuracy, can be misleading in imbalanced datasets. If the majority class dominates the dataset, a naive model that always predicts the majority class can achieve high accuracy without actually learning meaningful patterns.\n",
    "\n",
    "3. Reduced Generalization: Imbalanced data can hinder the generalization capability of a model. It may struggle to accurately classify minority class instances in real-world scenarios because it hasn't learned enough about them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two common techniques used to address class imbalance in a dataset. Let's explain each technique and provide examples of when they are required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Up-sampling:\n",
    "\n",
    "Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This is done by replicating or creating new synthetic samples from the existing minority class data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example:\n",
    "\n",
    " Suppose you have a dataset for credit card fraud detection, where the positive class (fraudulent transactions) is the minority class, and the negative class (non-fraudulent transactions) is the majority class. In this case, up-sampling can be used to generate additional instances of fraudulent transactions by replicating or generating synthetic samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Down-sampling:\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. This is done by randomly selecting a subset of the majority class data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example:\n",
    "\n",
    " Let's consider a medical dataset for cancer diagnosis, where the positive class (cancer patients) is the minority class, and the negative class (non-cancer patients) is the majority class. In this scenario, down-sampling can be used to randomly select a subset of non-cancer patient samples, reducing the number of instances to match the number of cancer patient samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a technique commonly used in machine learning and deep learning to artificially increase the size of a training dataset by creating new synthetic samples. It involves applying various transformations or modifications to existing data points, resulting in additional samples that are similar to the original data but exhibit some variation.\n",
    "\n",
    "\n",
    " * Here's an explanation of the SMOTE algorithm:\n",
    "\n",
    "\n",
    "1. Identify the minority class: SMOTE requires a dataset with a minority class (the class with fewer instances) and a majority class (the class with more instances).\n",
    "\n",
    "2. Select a minority class instance: Randomly pick an instance from the minority class.\n",
    "\n",
    "3. Find its k nearest neighbors: Calculate the distances between the selected instance and all other instances in the minority class. Choose the k nearest neighbors based on a distance metric, typically Euclidean distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Outliers are data points in a dataset that significantly deviate from the majority of the other data points. They are observations that lie an abnormal distance away from other data points and may exhibit extreme values or unusual characteristics.\n",
    "\n",
    "\n",
    "*  Here are a few reasons why handling outliers is essential:\n",
    "\n",
    "1. Distortion of Statistical Analysis:\n",
    "\n",
    " Outliers can greatly affect statistical measures such as the mean and standard deviation. The mean is highly sensitive to extreme values, causing it to be skewed towards the outliers. \n",
    "\n",
    " 2. Impact on Model Performance:\n",
    " \n",
    "  Outliers can have a detrimental effect on the performance of machine learning models. Models are designed to learn patterns and make predictions based on the majority of the data. Outliers, being different from the majority, can unduly influence the model's learning process, leading to poor generalization and reduced predictive accuracy.\n",
    "\n",
    "  \n",
    "3. Biased Estimates:\n",
    "\n",
    " Outliers can introduce bias into parameter estimates and statistical models. For instance, in linear regression, outliers can result in a biased estimation of regression coefficients, leading to unreliable predictions and incorrect inferences about the relationships between variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some commonly used techniques:\n",
    "\n",
    "1. Deletion:\n",
    "   - Listwise Deletion: Remove entire rows or instances with missing values. This approach eliminates the incomplete records but may result in a loss of information if the missing data is not completely random.\n",
    "   - Pairwise Deletion: Analyze available data for each specific analysis, ignoring missing values for individual variables. This approach allows for the maximum use of available data but may introduce bias due to the selective analysis.\n",
    "\n",
    "2. Mean/Median/Mode Imputation:\n",
    "   - Mean Imputation: Replace missing values with the mean of the available values for that variable. This approach assumes the data follows a normal distribution and may distort the variable's true distribution.\n",
    "   - Median Imputation: Replace missing values with the median of the available values for that variable. This approach is robust to outliers and works well for variables with skewed distributions.\n",
    "   - Mode Imputation: Replace missing categorical values with the mode (most frequent value) of the available values for that variable.\n",
    "\n",
    "3. Regression Imputation:\n",
    "   - Predictive Models: Use predictive models, such as linear regression or decision trees, to estimate missing values based on other variables. The missing variable is treated as the dependent variable, and the other variables are used as predictors to train the model and predict the missing values.\n",
    "\n",
    "4. Multiple Imputation:\n",
    "   - Generate multiple imputed datasets using advanced imputation methods like MICE (Multivariate Imputation by Chained Equations). This technique creates several imputations, each capturing the uncertainty of the missing values. The analyses are performed on each imputed dataset, and the results are combined to obtain overall estimates and standard errors.\n",
    "\n",
    "5. Domain Knowledge and Expert Input:\n",
    "   - Seek expert knowledge or input to estimate missing values based on contextual information or domain expertise. This approach can be useful when the missing values are challenging to impute using statistical methods alone.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with missing data in a large dataset, it is essential to determine whether the missingness is random or if there is a pattern or mechanism behind it. Here are some strategies you can use to assess the missing data pattern:\n",
    "\n",
    "1. Missing Data Visualization:\n",
    "   - Missingness Matrix: Create a missingness matrix or heatmap that visually represents the presence and patterns of missing values across variables. This matrix helps identify any noticeable patterns or correlations between missing values in different variables.\n",
    "   -\n",
    "\n",
    "2. Missing Data Mechanism Assessment:\n",
    "   - Missing Completely at Random (MCAR): Perform statistical tests to determine if the missing data is MCAR. For example, you can compare the distribution of the observed values for a variable with the distribution of the missing values. If there is no significant difference, it suggests MCAR.\n",
    "   \n",
    "\n",
    "3. Imputation Evaluation:\n",
    "   - Compare Imputation Results: Apply different imputation techniques and compare the results. If the imputed values are similar or consistent across different imputation methods, it indicates that the missingness pattern does not heavily influence the imputed values.\n",
    "   \n",
    "\n",
    "4. Statistical Tests:\n",
    "   - Analyze Differences: Conduct statistical tests to compare the characteristics or distributions of observed and missing values for specific variables. This helps identify if there are significant differences that suggest a non-random missing data mechanism.\n",
    "   \n",
    "\n",
    "5. Expert Consultation:\n",
    "   - Seek input from domain experts or individuals familiar with the data to gain insights into the missing data pattern. They might provide valuable information about potential biases or patterns that are not apparent from statistical analysis alone.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset in a medical diagnosis project, where the majority of patients do not have the condition of interest, it is important to adopt appropriate strategies to evaluate the performance of machine learning models.\n",
    "\n",
    "1. Confusion Matrix and Class-Specific Metrics:\n",
    "   - Use a confusion matrix to assess the performance of the model. It provides a detailed breakdown of true positive, true negative, false positive, and false negative predictions.\n",
    "   - Focus on class-specific metrics such as precision, recall, and F1-score. \n",
    "\n",
    "2. Resampling Techniques:\n",
    "   - Upsampling: Increase the number of samples in the minority class by randomly replicating existing samples or generating synthetic samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique). \n",
    "   .\n",
    "   - Downsampling: Reduce the number of samples in the majority class by randomly selecting a subset of samples. This can help prevent the model from being dominated by the majority class and improve its ability to identify the minority class.\n",
    "\n",
    "3. Weighted or Balanced Classifiers:\n",
    "   - Use machine learning algorithms that have built-in mechanisms to handle imbalanced data, such as weighted or balanced classifiers. These algorithms assign higher weights or adjust the decision thresholds to account for the class imbalance, giving more importance to the minority class during training and prediction.\n",
    "\n",
    "4. Ensemble Methods:\n",
    "   - Employ ensemble methods such as bagging or boosting techniques to improve the model's performance. These methods combine multiple models to make predictions, reducing the impact of individual models' biases and improving overall predictive accuracy, especially for the minority class.\n",
    "\n",
    "5. Cost-Sensitive Learning:\n",
    "   - Incorporate costs or misclassification penalties into the model training process. Assign higher costs or penalties to misclassifications of the minority class to encourage the model to focus more on correctly identifying the minority class instances.\n",
    "\n",
    "6. Cross-Validation and Stratified Sampling:\n",
    "   - Use stratified sampling during cross-validation to ensure that each fold contains a proportional representation of the minority class. This prevents over-optimistic or biased model performance estimates.\n",
    "\n",
    "7. Threshold Adjustment:\n",
    "   - Adjust the decision threshold of the model to achieve a desired balance between precision and recall. By moving the threshold, you can prioritize either reducing false positives or false negatives based on the specific requirements of the medical diagnosis problem.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here are some methods you can use to down-sample the majority class:\n",
    "\n",
    "1. Random Under-Sampling:\n",
    "   - Randomly select a subset of samples from the majority class to match the number of samples in the minority class. This approach may discard some data, potentially leading to information loss, but it can help balance the dataset.\n",
    "\n",
    "2. Cluster-Based Under-Sampling:\n",
    "   - Apply clustering algorithms such as K-means or DBSCAN to identify clusters within the majority class. Then, select representative samples from each cluster or use cluster centroids to down-sample the majority class. \n",
    "\n",
    "3. Tomek Links:\n",
    "   - Identify Tomek links, which are pairs of samples from different classes that are closest to each other. Remove the majority class samples from these pairs to create a down-sampled dataset. \n",
    "\n",
    "4. NearMiss Algorithm:\n",
    "   - NearMiss is an under-sampling technique that selects samples from the majority class based on their distance to the minority class samples. There are different versions of the NearMiss algorithm, such as NearMiss-1, NearMiss-2, and NearMiss-3, each with varying strategies for selecting the samples to be removed.\n",
    "\n",
    "5. Edited Nearest Neighbors:\n",
    "   - Use the Edited Nearest Neighbors (ENN) algorithm to identify samples from the majority class that are misclassified by their nearest neighbors from the same class.\n",
    "\n",
    "6. Instance Hardness Threshold:\n",
    "   - Calculate the hardness scores for each sample, which measure how difficult it is to classify a sample. Set a threshold and remove samples from the majority class with hardness scores above the threshold.\n",
    "\n",
    "7. Combination of Over-Sampling and Under-Sampling:\n",
    "   - Perform a combination of over-sampling techniques (e.g., SMOTE) on the minority class and under-sampling techniques (e.g., random under-sampling) on the majority class to achieve a more balanced dataset. This approach helps maintain the representation of both classes while reducing the class imbalance.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here are some methods you can use to up-sample the minority class:\n",
    "\n",
    "1. Random Over-Sampling:\n",
    "   - Randomly duplicate samples from the minority class to increase their representation. This approach increases the occurrence of the rare event but may lead to overfitting if the duplicated samples introduce too much redundancy.\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "   - Generate synthetic samples by interpolating between feature vectors of minority class instances. SMOTE selects a minority class instance, finds its k nearest neighbors, and creates synthetic samples along the line segments connecting them. \n",
    "\n",
    "3. ADASYN (Adaptive Synthetic Sampling):\n",
    "   - ADASYN is an extension of SMOTE that adapts the distribution of synthetic samples based on the difficulty of learning from minority class instances. \n",
    "\n",
    "4. SMOTE-ENN:\n",
    "   - Combine SMOTE (over-sampling) with ENN (Edited Nearest Neighbors) (under-sampling). First, apply SMOTE to generate synthetic samples for the minority class. Then, use ENN to remove any misclassified samples from both the majority and minority classes. \n",
    "\n",
    "5. SMOTE-Tomek Links:\n",
    "   - Combine SMOTE with Tomek Links, which are pairs of samples from different classes that are closest to each other. SMOTE is applied to the minority class, and Tomek Links are used to identify and remove the overlapping instances from the majority class. \n",
    "\n",
    "6. Ensemble-Based Methods:\n",
    "   - Utilize ensemble methods like EasyEnsemble, BalanceCascade, or RUSBoost that create multiple balanced subsets of the data by resampling the minority class or using different combinations of over-sampling and under-sampling techniques. \n",
    "\n",
    "7. Synthetic Data Generation:\n",
    "   - If the dataset is limited or the minority class is extremely rare, you can consider generating synthetic data using generative models such as Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
